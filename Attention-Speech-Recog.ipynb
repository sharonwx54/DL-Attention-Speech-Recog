{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "h7Rd-7SJEKX-",
        "-njBvl2Opd6I"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "<img src=\"https://cdn.shopify.com/s/files/1/0272/2080/3722/products/SmileBumperSticker_5400x.jpg\" alt=\"A cute cat\" width=\"600\">\n",
        "\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with <i>attention</i>. <br> <br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://popmn.org/wp-content/uploads/2020/03/pay-attention.jpg\" alt=\"A cute cat\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "HW Writeup: On Piazza/Course Website <br>\n",
        "Kaggle Competition Link: https://www.kaggle.com/competitions/11-785-s23-hw4p2/ <br>\n",
        "Kaggle Dataset Link: https://www.kaggle.com/datasets/varunjain3/11-785-s23-hw4p2-dataset\n",
        "<br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ],
      "metadata": {
        "id": "8XpNMS7Vk6Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruction to Run the Code"
      ],
      "metadata": {
        "id": "1JvRTZr5XuXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To run the final model corresponding to the highest kaggle submission, please first make sure the **Global Variables** (next section) are set to fit the purpose, and then go to **Runtime**, click **Restart and run all**. This would\n",
        "1. pip install, import and download all required packages and data\n",
        "2. run all functions and classes for loading data and creating model/optimizer/scheduler\n",
        "3. train the model for 100 epochs based on the parameters saved in the config variable defined under **LibriSpeech**.\n",
        "4. Sequentially finetune the model for 50 epoch with learning rates reset back slightly (0.005)\n",
        "\n",
        "**Note** that by default, the notebook is expected to finish running in one click. If you pause the run and want to reload the model from saved path for finetuning, please set **RELOAD** as True under Training section and enter reload path.\n",
        "\n",
        "**Note** that by default, the notebook would reinstall all packages that might take a long time. If you are using GCP / AWS to run it on a previously run section, please set **REINSTALL** to be False.\n",
        "\n",
        "**Note** that by default, the notebook would run the trained model on the test dataset and save the predicted result in csv file, but it would not make the submission to Kaggle. To run the notebook with kaggle submission, set **SUBMIT_KAGGLE** to True."
      ],
      "metadata": {
        "id": "QHjvzwHAXwxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "swbrdQyMYMW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_TOY = False\n",
        "CONNECT_DRIVE = False\n",
        "REINSTALL = True\n",
        "SUBMIT = False"
      ],
      "metadata": {
        "id": "QMnJwing8Edw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README"
      ],
      "metadata": {
        "id": "SjiOlPaJYf_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Score Hyperparameters:\n",
        "* **init learning rate** = 2e-3\n",
        "* **finetune init learning rate** = 5e-4\n",
        "* **teacher forcing ratio** = init 1.0 and drop 0.1 every 20 epochs until hitting 0.5\n",
        "* **encoder embedding batchnorm**: after every Conv1D layer\n",
        "* **encoder embedding activate**: Not Applicable\n",
        "* **encoder LSTM dim** = (256, 256)\n",
        "* **encoder LSTM dropout** = 0.5\n",
        "* **encoder LSTM layers** = 1\n",
        "* **encoder pLSTM locked dropout** = 0.5 for the first two blocks and 0.25 for the last one\n",
        "* **encoder pLSTM layers** = 3\n",
        "* **encoder pLSTM dim** = (1024, 256) > (1024, 256) > (1024, 256)\n",
        "* **encoder hidden size** = 256\n",
        "* **encoder embed size** = 256\n",
        "* **attention projection size** = 128\n",
        "* **attention mask value** = -1e9\n",
        "* **decoder LSTM Cell layers**: 3\n",
        "* **decoder LSTM Cell dim**: (256+128, 512) > (512, 512) > (512, 128)\n",
        "* **decoder LSTM Cell dropout** = 0.25\n",
        "* **decoder CDN linear layers**: 2\n",
        "* **decoder CDN activation**: Tanh\n",
        "* **decoder CDN dim**: (256, 512) > (512, 256) > (256, 31)\n",
        "* **decoder hidden size** = 512\n",
        "* **decoder embed size** = 256\n",
        "* **weight decay** = None\n",
        "* **batch size** = 192\n",
        "* **epoch** = 100\n",
        "* **finetune epoch** = 50\n",
        "* **weight init**: None\n",
        "* **scheduler**: ReduceLROnPlateau <patience = 3, factor = 0.8, mode = min>\n",
        "* **optimizer**: Adam\n",
        "\n",
        "\n",
        "\n",
        "## Data Loading Scheme:\n",
        "The highest kaggle score is run by loading and training on the train 100 dataset using **SpeechDataset**, where all mfcc files are read in order and saved into a list for the final concatenation. No memory handling is used.\n",
        "\n",
        "All MFCCs data are normalized using Cepstral Normalization, but note that all Transcriptions data DO NOT have EOS and SOS removed in this assignment. No other data transform is used.\n",
        "\n",
        "\n",
        "## Architectures:\n",
        "The highest kaggle score is reached using LAS model with encoder, attention, and decoder.\n",
        "\n",
        "For encoder, the embedding contains only a Conv1D with kernel size 3 and a batchnorm followed. No activation is used. After the embedding, data are passed into a one-layer biLSTM with 0.5 dropout. The input size is the embed size (256) and output size is encoder hidden size (256). Note that the dropout is not directly added in this LSTM layer. Instead , it is call in the pBiLSTM layer before the input data is passed into the pBiLSTM network. This is due to the design of the pBiLSTM class.  \n",
        "\n",
        "Then, the architecture includes three pBiLSTMs block. For all three blocks, before the the pBiLSTM is applied, a LockedDropout layer with 0.5 rate is called first. After the last pBiLSTM, a LockedDropout with 0.25 rate is called. The encoder hidden size (256) is used for pBiLSTM output size.\n",
        "\n",
        "For attention, the input encoder hidden size is 512, a double of the output from listener/encoder. It applied linear transformation for the encoder output with (512, 128) dimension to generate key and value. Attention mask is used with filled value -1e9. The mask has the shape (batch-size, time-steps) and is applied before the attention weights are passed into the softmax.\n",
        "\n",
        "For decoder, the architecture is a one-layer embedding, 3 layer LSTM-Cell, followed by a 2 layer MLP, and a final linear output layer. Each LSTM Cell is followed by a LockedDropout with 0.25 rate, except for the last layer. For each time-step, The output from the embedding of the time-step is first concatenated with attention value from the previous time-step, then passed into the attention as input to compute the query using linear transformation, and then the query is used to compute a new attention context. The new context goes into the LSTM cells, after which the LSTM output is passed into the attention again, but this time the LSTM output is served as query, so no new query is computed, and the LSTM output is used to compute a new attention context. The output context is then passed into the CDN block with LSTM output. The CDN block project the concatenated input with 256 dimension to 512 first, then Tanh is applied, and another linear transformation is 512 back to 256 is applied, followed by another Tanh activation, before the final output layer.  \n",
        "\n",
        "Note that the final version passed different input values into the attention twice, one time compute a new query for attention context and one time using LSTM cell output as query.\n",
        "\n",
        "Other architectures are tested as well, but with less ideal performance:\n",
        "1. For encoder, a 2-layer pBiLSTMs is attempted, and the model cannot converge.\n",
        "2. For encoder, a 2-layer biLSTM + 2-layer pBiLSTMs is attempted, but the performance does not improved.\n",
        "3. For encoder, a more complicated embedding with 2 convolution blocks and activation are attempted, and the model cannot converge.\n",
        "4. For decoder, a 1 layer MLP is attempted.\n",
        "5. For decoder, a 2-layer LSTM-cell is attempt, using LSTM cell output directly as query. The model could converge but slower.\n",
        "6. For decoder, a 2-layer LSTM-cell + 2-layer CDN is attempt, only computing query inside attention rather than use LSTM output as query.\n",
        "7. For attention, a version without masking is attempted. The model converge slower.\n",
        "8. For LAS, a model with all hidden, embed, and projection size doubled is attempted, and the model converge much slower.  \n",
        "\n",
        "## Epochs\n",
        "I trained the model for 100 epochs and finetuned for another 50 epochs to get to my highest kaggle submission. The model start to show distance decrease at around epoch 8, converging at about 80 epoch, but the value vacillate a bit. After 100 epoch, I reset the learning rate for finetuning again. At about 11 epoch, the model hit the lowest distance I could get. I further finetuned it until 50 epochs are finished, but no improved is made.\n",
        "\n",
        "\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "* **Batch Size**: 192. Given the computation capacity of my GCP, the maximum batch size I could use is 192 for the given model.\n",
        "\n",
        "* **Weight Decay**: None. I started with  5e-5 weight decay based on the previous homework, but during the ablation on toy dataset, the model converge faster without decay.\n",
        "\n",
        "* **Dropout**: on encoder, I attempted locked dropout rate of 0.25 and 0.5, and find 0.5 the best rate in terms of performance. On decoder, I a attempted 0.35 based on HW1 and 0.25 , and the 0.25 leads to faster convergence on toy dataset.\n",
        "\n",
        "* **hidden size**: I started with the basic parameters given by the handout, and attempted doubling the hidden size of encoder only, and of encoder and decode. A larger hidden size make the model fail to converge on LibriSpeech dataset.\n",
        "\n",
        "\n",
        "\n",
        "## Other Experiments\n",
        "### Activation Function on Decoder CDN\n",
        "I selected the LeakyReLU at first based on recommendation online, but it seems that Tanh works much better in this homework.\n",
        "\n",
        "## Encoder Embedding\n",
        "I tried complicated embedding of encoder, from ResNet to 2 and 3 layer CNN with batchnorm and activation. However, my model with only 1 Conv layer and 1 batchnorm layer perform the best.\n",
        "\n",
        "### Scheduler and Learning Rate\n",
        "I selected ReduceLROnPlateau with patience 3 and factor 0.8, with min mode. I started with using patience = 1 based on HW3, but the attention model tends to fluctutate in the distance, and a too low patience is not ideal. I think increase the patience to allow the model to train a bit more before adjusting to a lower learning rate. In addition, since the distance does not start to decrease until epoch 8 - 10, I only use the scheduler after 15 epoch, to prevent to learning rate from decreasing too early.\n",
        "\n",
        "### Optimizer\n",
        "I only switched between AdamW and Adam optimizer, and found Adam better without weight decay.\n",
        "\n"
      ],
      "metadata": {
        "id": "kkzLpWM2Yh62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read this section importantly!"
      ],
      "metadata": {
        "id": "vwIdDTTmmZVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. By now, we believe that you are already a great deep learning practitioner, Congratulations. 🎉\n",
        "\n",
        "2. You are allowed to use code from your previous homeworks for this homework. We will only provide, aspects that are necessary and new with this homework.\n",
        "\n",
        "3. There are a lot of resources provided in this notebook, that will help you check if you are running your implementations correctly."
      ],
      "metadata": {
        "id": "y9qsVrRemgh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8UK7J-dp5iN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0833216-7a09-42df-e6d7-5dcaa0825427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 21 04:58:05 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp"
      ],
      "metadata": {
        "id": "nYgaLmgy5iqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "yEkA_GGG-tTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Modules you require for this HW here\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.decomposition import PCA\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import Levenshtein\n",
        "\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ],
      "metadata": {
        "id": "p0aXmrxM-usO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONNECT_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaG35t53EqON",
        "outputId": "4b0ff660-7f5a-4a04-f396-baf6a163205d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Dataset Download"
      ],
      "metadata": {
        "id": "h7Rd-7SJEKX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY and REINSTALL:\n",
        "  !wget -q https://cmu.box.com/shared/static/om4qpzd4tf1xo4h7230k4v1pbdyueghe --content-disposition --show-progress\n",
        "  !unzip -q hw4p2_toy.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvEV0KwEKDX",
        "outputId": "aeccb032-4ac7-4b15-93db-88f8bdf60ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "unzip:  cannot find or open hw4p2_toy.zip, hw4p2_toy.zip.zip or hw4p2_toy.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy Dataset"
      ],
      "metadata": {
        "id": "G_8wdpm3WrT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The toy dataset is a dataset of fixed length speech sequences that have phonetic transcripts. The reason we made it with phonetic transcripts was to help you understand how attention can work with phonetic transcription that you have done in HW3P2"
      ],
      "metadata": {
        "id": "kCzEdHVZWyga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY:\n",
        "  # Load the toy dataset\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  X_train = np.load(\"hw4p2_toy/f0176_mfccs_train_new.npy\")\n",
        "  X_valid = np.load(\"hw4p2_toy/f0176_mfccs_dev_new.npy\")\n",
        "  Y_train = np.load(\"hw4p2_toy/f0176_hw3p2_train.npy\")\n",
        "  Y_valid = np.load(\"hw4p2_toy/f0176_hw3p2_dev.npy\")\n",
        "\n",
        "  # This is how you actually need to find out the different trancripts in a dataset.\n",
        "  # Can you think whats going on here? Why are we using a np.unique?\n",
        "  VOCAB_MAP_TOY           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid)))))\n",
        "  VOCAB_MAP_TOY[\"[PAD]\"]  = len(VOCAB_MAP_TOY)\n",
        "  VOCAB_TOY               = list(VOCAB_MAP_TOY.keys())\n",
        "\n",
        "  SOS_TOKEN_TOY = VOCAB_MAP_TOY[\"[SOS]\"]\n",
        "  EOS_TOKEN_TOY = VOCAB_MAP_TOY[\"[EOS]\"]\n",
        "  PAD_TOKEN_TOY = VOCAB_MAP_TOY[\"[PAD]\"]\n",
        "\n",
        "  Y_train = [np.array([VOCAB_MAP_TOY[p] for p in seq]) for seq in Y_train]\n",
        "  Y_valid = [np.array([VOCAB_MAP_TOY[p] for p in seq]) for seq in Y_valid]"
      ],
      "metadata": {
        "id": "oLhnu3Y2WvFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY:\n",
        "  VOCAB = VOCAB_TOY\n",
        "\n",
        "  VOCAB_MAP = VOCAB_MAP_TOY\n",
        "\n",
        "  PAD_TOKEN = PAD_TOKEN_TOY\n",
        "  SOS_TOKEN = SOS_TOKEN_TOY\n",
        "  EOS_TOKEN = EOS_TOKEN_TOY\n",
        "\n",
        "  print(f\"Length of vocab: {len(VOCAB)}\")\n",
        "  print(f\"Vocab: {VOCAB}\")\n",
        "  print(f\"PAD_TOKEN: {PAD_TOKEN}\")\n",
        "  print(f\"SOS_TOKEN: {SOS_TOKEN}\")\n",
        "  print(f\"EOS_TOKEN: {EOS_TOKEN}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2uq9UH4VZaV",
        "outputId": "d12e70c9-a95f-45aa-b177-901cc7dc06bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 43\n",
            "Vocab: ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', '[EOS]', '[SIL]', '[SOS]', '[PAD]']\n",
            "PAD_TOKEN: 42\n",
            "SOS_TOKEN: 41\n",
            "EOS_TOKEN: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY:\n",
        "  class ToyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "      def __init__(self, partition):\n",
        "\n",
        "          if partition == \"train\":\n",
        "              self.mfccs = X_train\n",
        "              self.transcripts = Y_train\n",
        "\n",
        "          elif partition == \"valid\":\n",
        "              self.mfccs = X_valid\n",
        "              self.transcripts = Y_valid\n",
        "\n",
        "          assert len(self.mfccs) == len(self.transcripts)\n",
        "\n",
        "          self.length = len(self.mfccs)\n",
        "\n",
        "      def __len__(self):\n",
        "\n",
        "          return self.length\n",
        "\n",
        "      def __getitem__(self, i):\n",
        "\n",
        "          x = torch.FloatTensor(self.mfccs[i])\n",
        "          y = torch.tensor(self.transcripts[i])\n",
        "\n",
        "          return x, y\n",
        "\n",
        "      def collate_fn(self, batch):\n",
        "\n",
        "          x_batch, y_batch = list(zip(*batch))\n",
        "\n",
        "          x_lens      = [x.shape[0] for x in x_batch]\n",
        "          y_lens      = [y.shape[0] for y in y_batch]\n",
        "\n",
        "          x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN_TOY)\n",
        "          y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN_TOY)\n",
        "\n",
        "          return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
      ],
      "metadata": {
        "id": "U5ub4PLbWsaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY:\n",
        "    config = {'init_lr': 0.001}\n",
        "    config['batch_size'] = 64\n",
        "    train_toy_dataset   = ToyDataset(partition= 'train')\n",
        "    valid_toy_dataset   = ToyDataset(partition= 'valid')\n",
        "\n",
        "    train_toy_loader    = torch.utils.data.DataLoader(\n",
        "        dataset     = train_toy_dataset,\n",
        "        batch_size  = config['batch_size'],\n",
        "        shuffle     = True,\n",
        "        num_workers = 4,\n",
        "        pin_memory  = True,\n",
        "        collate_fn  = train_toy_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    valid_toy_loader    = torch.utils.data.DataLoader(\n",
        "        dataset     = valid_toy_dataset,\n",
        "        batch_size  = config['batch_size'],\n",
        "        shuffle     = False,\n",
        "        num_workers = 2,\n",
        "        pin_memory  = True,\n",
        "        collate_fn  = valid_toy_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    train_loader = train_toy_loader\n",
        "    dev_loader = valid_toy_loader\n",
        "    print(\"No. of train mfccs   : \", train_toy_dataset.__len__())\n",
        "    print(\"Batch size           : \", config['batch_size'])\n",
        "    print(\"Train batches        : \", train_toy_loader.__len__())\n",
        "    print(\"Valid batches        : \", valid_toy_loader.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpirO0MZnON",
        "outputId": "b9b86c9f-5b4e-4a0c-da39-ea7101b1c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of train mfccs   :  16000\n",
            "Batch size           :  128\n",
            "Train batches        :  125\n",
            "Valid batches        :  13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_TOY:\n",
        "  for batch in valid_toy_loader:\n",
        "      x, y, x_len, y_len = batch\n",
        "      print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXW48FH8VIOC",
        "outputId": "5a687286-1bc6-46ca-a17c-7f772fe59b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 176, 27]) torch.Size([128, 23]) torch.Size([128]) torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Dataset Download"
      ],
      "metadata": {
        "id": "-njBvl2Opd6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if REINSTALL:\n",
        "    api_token = '{\"username\":\"sharonxin1207\",\"key\":\"a6eb67109ee97e7f02df4bfe642cf615\"}'\n",
        "\n",
        "    # set up kaggle.json\n",
        "    # TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
        "    !mkdir /root/.kaggle/\n",
        "\n",
        "    with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "        f.write(api_token) # Put your kaggle username & key here\n",
        "\n",
        "    !chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PTyWR2sIp0Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if REINSTALL:\n",
        "  # To download the dataset\n",
        "  !kaggle datasets download -d varunjain3/11-785-s23-hw4p2-dataset"
      ],
      "metadata": {
        "id": "F581gjfnqE2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2417009a-0ae5-49c2-cf2f-4ec411567420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s23-hw4p2-dataset.zip to /content\n",
            "100% 3.73G/3.74G [00:38<00:00, 117MB/s]\n",
            "100% 3.74G/3.74G [00:38<00:00, 103MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if REINSTALL:\n",
        "  # To unzip data quickly and quietly\n",
        "  !unzip -q 11-785-s23-hw4p2-dataset.zip -d ./data"
      ],
      "metadata": {
        "id": "7ko7QN16qF2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloaders\n",
        "\n",
        "We have given you 2 datasets. One is a toy dataset, and the other is the standard LibriSpeech dataset. The toy dataset is to help you get your code implemented and tested and debugged easily, to verify that your attention diagonal is produced correctly. Note however that it's task (phonetic transcription) is drawn from HW3P2, it is meant to be familiar and help you understand how to transition from phonetic transcription to alphabet transcription, with a working attention module.\n",
        "\n",
        "Please make sure you use the right constants in your code implementation for future modules, (SOS_TOKEN vs SOS_TOKEN_TOY) when working with either dataset. We have defined the constants accordingly below. Before you come to OH or post on piazza, make sure you aren't misuing the constants for either dataset in your code."
      ],
      "metadata": {
        "id": "zUJyBBwIqQs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LibriSpeech\n",
        "\n",
        "In terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required??\n",
        "\n",
        "Hints:\n",
        "\n",
        "- Check how big is the dataset (do you require memory efficient loading techniques??)\n",
        "- How do we load mfccs? Do we need to normalise them?\n",
        "- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n",
        "- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n",
        "- Observe the VOCAB, is the dataset same as HW3P2?\n",
        "- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n"
      ],
      "metadata": {
        "id": "1sjsbFuKWp7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBMLGYX-kZcd",
        "outputId": "0683ad89-466c-4022-d11f-5137d9d7b6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 31\n",
            "Vocab: ['<pad>', '<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ']\n",
            "PAD_TOKEN: 0\n",
            "SOS_TOKEN: 1\n",
            "EOS_TOKEN: 2\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "  'batch_size': 192, #128,\n",
        "  'lr':2e-3,\n",
        "  'epochs': 100,\n",
        "  'finetune_epochs': 50\n",
        "}\n",
        "\n",
        "root = \"/content/data\"\n",
        "\n",
        "VOCAB = ['<pad>', '<sos>', '<eos>',\n",
        "         'A',   'B',    'C',    'D',\n",
        "         'E',   'F',    'G',    'H',\n",
        "         'I',   'J',    'K',    'L',\n",
        "         'M',   'N',    'O',    'P',\n",
        "         'Q',   'R',    'S',    'T',\n",
        "         'U',   'V',    'W',    'X',\n",
        "         'Y',   'Z',    \"'\",    ' ',\n",
        "         ]\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "\n",
        "print(f\"Length of vocab: {len(VOCAB)}\")\n",
        "print(f\"Vocab: {VOCAB}\")\n",
        "print(f\"PAD_TOKEN: {PAD_TOKEN}\")\n",
        "print(f\"SOS_TOKEN: {SOS_TOKEN}\")\n",
        "print(f\"EOS_TOKEN: {EOS_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, partition='train-clean-100', transform=[\"norm\"]):\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = \"{}/{}/mfcc\".format(root, partition)\n",
        "        self.transcript_dir = \"{}/{}/transcripts\".format(root, partition)\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        self.mapping = VOCAB_MAP\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+self.mfcc_files[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            if \"norm\" in self.transform:\n",
        "                mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(self.transcript_dir+\"/\"+self.transcript_files[i])\n",
        "            transcript = np.array([self.mapping[t] for t in transcript])\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind])\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [b[0] for b in batch]\n",
        "        batch_transcript = [b[1] for b in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True, padding_value=PAD_TOKEN) # TODO\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True, padding_value=PAD_TOKEN) # TODO\n",
        "        lengths_transcript = [len(trans) for trans in batch_transcript]\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n"
      ],
      "metadata": {
        "id": "VuneWaTStdF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataloader\n",
        "class SpeechTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n",
        "    def __init__(self, root, partition= \"test-clean\", transform=[\"norm\"]): # Feel free to add more arguments\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = \"{}/{}/mfcc\".format(root, partition)\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.mfccs = []\n",
        "        self.transform = transform\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+mfcc_names[i])\n",
        "            if \"norm\" in self.transform:\n",
        "                mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "            self.mfccs.append(mfcc)\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind]) # Convert to tensors\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        batch_mfcc_pad = pad_sequence(batch, batch_first=True, padding_value=PAD_TOKEN) # TODO\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n"
      ],
      "metadata": {
        "id": "UU-Co6ubUz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset = SpeechDataset(root, 'dev-clean')\n",
        "train_dataset = SpeechDataset(root, 'train-clean-100')\n",
        "test_dataset = SpeechTestDataset(root)\n",
        "\n",
        "dev_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = dev_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    collate_fn  = dev_dataset.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        "\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    collate_fn  = train_dataset.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_dataset,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    collate_fn  = test_dataset.collate_fn,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in dev_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "tzuIXCyAuNvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286a4d98-a1a7-4a9b-c395-4fb44331387b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([192, 2936, 27]) torch.Size([192, 364]) torch.Size([192]) torch.Size([192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if you are loading the data correctly with the following:\n",
        "\n",
        "(Note: These are outputs from loading your data in the dataset class, not your dataloader which will have padded sequences)\n",
        "\n",
        "- Train Dataset\n",
        "```\n",
        "Partition loaded:  train-clean-100\n",
        "Max mfcc length:  2448\n",
        "Average mfcc length:  1264.6258453344547\n",
        "Max transcript:  400\n",
        "Average transcript length:  186.65321139493324\n",
        "```\n",
        "\n",
        "- Dev Dataset\n",
        "```\n",
        "Partition loaded:  dev-clean\n",
        "Max mfcc length:  3260\n",
        "Average mfcc length:  713.3570107288198\n",
        "Max transcript:  518\n",
        "Average transcript length:  108.71698113207547\n",
        "```\n",
        "\n",
        "- Test Dataset\n",
        "```\n",
        "Partition loaded:  test-clean\n",
        "Max mfcc length:  3491\n",
        "Average mfcc length:  738.2206106870229\n",
        "```\n",
        "\n",
        "If your values is not matching, read hints, think what could have gone wrong. Then approach TAs."
      ],
      "metadata": {
        "id": "i_n3pqt7ud4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE MODEL\n",
        "\n",
        "### Listen, Attend and Spell\n",
        "Listen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n",
        "\n",
        "- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n",
        "- LAS is known for its high accuracy and ability to improve over time with additional training data.\n",
        "- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n",
        "\n",
        "#### The Dataflow:\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "#### The Listener:\n",
        "- converts the input speech signal into a sequence of hidden states.\n",
        "\n",
        "#### The Attender:\n",
        "- Decides how the sequence of Encoder hidden state is propogated to decoder.\n",
        "\n",
        "#### The Speller:\n",
        "- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8q9wt4TwzPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks"
      ],
      "metadata": {
        "id": "Q1YWihYh0IN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, torch.nn.Conv1d):\n",
        "        torch.nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n",
        "    elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "        torch.nn.init.constant_(m.weight.data, 1)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "ZT2G1kFp0OOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(torch.nn.Sequential):\n",
        "    def __init__(self, in_chan, out_chan, kernel, stride, padding=-1, groups=1, bias=False):\n",
        "        if padding < 0:\n",
        "          padding = (kernel-1)//2\n",
        "        super(ConvBlock, self).__init__(\n",
        "            torch.nn.Conv1d(in_channels=in_chan, out_channels=out_chan,\n",
        "                            kernel_size=kernel, stride=stride, padding=padding,\n",
        "                            groups=groups, bias=bias),\n",
        "            torch.nn.BatchNorm1d(out_chan),\n",
        "            #torch.nn.ReLU6(inplace=True)\n",
        "            torch.nn.GELU()\n",
        "        )"
      ],
      "metadata": {
        "id": "x2c1YE3j6-vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "class PLSTMLockedDropout(torch.nn.Module):\n",
        "  def __init__(self, dropout=0.5, lockafter=False):\n",
        "    super().__init__()\n",
        "    self.dropout = dropout\n",
        "    self.lockafter = lockafter\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: batch, seq_len, feature\n",
        "    if not self.training or not self.dropout:\n",
        "      return x\n",
        "    if self.lockafter:\n",
        "      x, x_lens = pad_packed_sequence(x, batch_first=True)\n",
        "    m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - self.dropout)\n",
        "    mask = Variable(m, requires_grad=False) / (1 - self.dropout)\n",
        "    mask = mask.expand_as(x).to(DEVICE)\n",
        "    mask_x = mask * x\n",
        "    if self.lockafter:\n",
        "      mask_x = pack_padded_sequence(mask_x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "    return mask_x\n",
        "\n",
        "\n",
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, droprate=0.5):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = torch.nn.LSTM(input_size*2, hidden_size, num_layers=1,\n",
        "                                   batch_first=True, bidirectional=True)\n",
        "        if droprate > 0:\n",
        "          self.dropout = PLSTMLockedDropout(droprate)\n",
        "        else:\n",
        "          self.dropout = None\n",
        "\n",
        "    def forward(self, x_packed):\n",
        "        x_pad, x_pad_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "        x_trunc, x_trunc_lens = self.trunc_reshape(x_pad, x_pad_lens)\n",
        "        if self.dropout:\n",
        "          x_trunc = self.dropout(x_trunc) # lock dropout layer here works better\n",
        "        x_packed = pack_padded_sequence(x_trunc, x_trunc_lens, batch_first=True, enforce_sorted=False)\n",
        "        output_pad, output_pad_lens = self.blstm(x_packed)\n",
        "        \"\"\"\n",
        "        if self.dropout:\n",
        "          output_pad, output_lens = pad_packed_sequence(output_pad, batch_first=True)\n",
        "          output_pad = self.dropout(output_pad)\n",
        "          output_pad = pack_padded_sequence(output_pad, output_lens, batch_first=True, enforce_sorted=False)\n",
        "        \"\"\"\n",
        "        return output_pad\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): # x: [b, seq, dim]\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        if x.shape[1] % 2 != 0:\n",
        "          x = x[:, :-1, :]\n",
        "        x_down = x.reshape(x.size(0), x.size(1) // 2, x.size(2)*2) # [b, seq/2, 2*dim]\n",
        "        x_lens = x_lens // 2\n",
        "\n",
        "        return x_down, x_lens"
      ],
      "metadata": {
        "id": "QWc5oPo1Iuiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Listener:"
      ],
      "metadata": {
        "id": "aPL08W_Z3R18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Psuedocode:\n",
        "```python\n",
        "class Listner:\n",
        "  def init():\n",
        "    feature_embedder = #Few layers of 1DConv-batchnorm-activation (Don't overdo)\n",
        "    pblstm_encoder = #Cascaded pblstm layers (Take pblstm from #HW3P2),\n",
        "    #can add more sequential lstms\n",
        "    dropout = #As per your liking\n",
        "\n",
        "  def forward(x,lx):\n",
        "    embedding = feature_embedder(x) #optional\n",
        "    encoding, encoding_len = pblstm_encoder(embedding/x,lx)\n",
        "    #Regularization if needed\n",
        "    return encoding, encoding_len\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_ewKlVQF3edC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Listener(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size, listener_hidden_size):\n",
        "        super(Listener, self).__init__()\n",
        "\n",
        "\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(input_size, embed_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(embed_size)\n",
        "        )\n",
        "        \"\"\"\n",
        "        last_embed = torch.nn.Sequential(\n",
        "            torch.nn.Conv1d(embed_size, embed_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            torch.nn.BatchNorm1d(embed_size)\n",
        "        )\n",
        "        torch.nn.Sequential(*[\n",
        "            ConvBlock(input_size, 256, 3, 1),\n",
        "            ConvBlock(256, embed_size, 3, 1),\n",
        "            last_embed]\n",
        "        )\n",
        "        \"\"\"\n",
        "        self.lstm = torch.nn.LSTM(embed_size, embed_size, num_layers=1, #dropout = 0.25,\n",
        "                                  batch_first=True, bidirectional=True)\n",
        "\n",
        "        self.pBLSTMs = torch.nn.Sequential(\n",
        "            pBLSTM(embed_size*2, listener_hidden_size),\n",
        "            pBLSTM(listener_hidden_size*2, listener_hidden_size),\n",
        "            pBLSTM(listener_hidden_size*2, listener_hidden_size),\n",
        "            #pBLSTM(listener_hidden_size*2, listener_hidden_size),\n",
        "            PLSTMLockedDropout(0.25, True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        embed_x_t = self.embedding(x.transpose(1, 2))\n",
        "        embed_x = embed_x_t.transpose(1, 2)\n",
        "\n",
        "        # Pass through the first LSTM at the very bottom\n",
        "        x_combined = pack_padded_sequence(embed_x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_output, _ = self.lstm(x_combined)\n",
        "\n",
        "        # TODO: Pass through the pBLSTM blocks\n",
        "        pblstm_output = self.pBLSTMs(lstm_output)\n",
        "\n",
        "        # Unpack the sequence\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(pblstm_output, batch_first=True)\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ],
      "metadata": {
        "id": "jTQHiB1jvM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query)\n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD\n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ],
      "metadata": {
        "id": "5fG9jDZBVklL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudocode:\n",
        "\n",
        "```python\n",
        "class Attention:\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    attention_weights   = softmax(raw_weights)\n",
        "    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n",
        "\n",
        "    At the end, you can pass context through a linear layer too.\n",
        "    '''\n",
        "\n",
        "    def init(listener_hidden_size,\n",
        "              speller_hidden_size,\n",
        "              projection_size):\n",
        "\n",
        "        VW = Linear(listener_hidden_size,projection_size)\n",
        "        KW = Linear(listener_hidden_size,projection_size)\n",
        "        QW = Linear(speller_hidden_size,projection_size)\n",
        "\n",
        "    def set_key_value(encoder_outputs):\n",
        "        '''\n",
        "        In this function we take the encoder embeddings and make key and values from it.\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        '''\n",
        "        key = KW(encoder_outputs)\n",
        "        value = VW(encoder_outputs)\n",
        "      \n",
        "    def compute_context(decoder_context):\n",
        "        '''\n",
        "        In this function from decoder context, we make the query, and then we\n",
        "         multiply the queries with the keys to find the attention logits,\n",
        "         finally we take a softmax to calculate attention energy which gets\n",
        "         multiplied to the generted values and then gets summed.\n",
        "\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        query.shape = (batch_size, projection_size)\n",
        "\n",
        "        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n",
        "        '''\n",
        "        query = QW(decoder_context) #(batch_size, projection_size)\n",
        "\n",
        "        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n",
        "        #What will be the shape of raw_weights?\n",
        "\n",
        "        attention_weights = #What makes raw_weights -> attention_weights\n",
        "\n",
        "        attention_context = #Multiply attention weights to values\n",
        "\n",
        "        return attention_context, attention_weights\n",
        "```"
      ],
      "metadata": {
        "id": "QyO03fO5VotY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, listener_hidden_size, speller_hidden_size, projection_size=128):\n",
        "        # 512, 512, 128\n",
        "        super().__init__()\n",
        "        self.batch_size = None # init\n",
        "        self.projection_size = projection_size\n",
        "\n",
        "        self.VW = torch.nn.Linear(listener_hidden_size, projection_size) # could have diff proj size than k and q\n",
        "        self.KW = torch.nn.Linear(listener_hidden_size, projection_size)\n",
        "        self.leakyrelu = torch.nn.LeakyReLU(negative_slope=0.25)\n",
        "        #self.QW = torch.nn.Linear(speller_hidden_size, projection_size)\n",
        "\n",
        "    def set_key_value(self, encoder_outputs):\n",
        "        '''\n",
        "        In this function we take the encoder embeddings and make key and values from it.\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        '''\n",
        "        self.key = self.KW(encoder_outputs)\n",
        "        self.value = self.VW(encoder_outputs)\n",
        "        self.batch_size = encoder_outputs.shape[0]\n",
        "\n",
        "        return self.key, self.value\n",
        "\n",
        "    def forward(self, decoder_context, attn_mask, compute_query=False, add_activate=False):\n",
        "\n",
        "        if compute_query:\n",
        "          self.QW = torch.nn.Linear(decoder_context.shape[1], self.projection_size).to(DEVICE)\n",
        "          self.query = self.QW(decoder_context) #(batch_size, projection_size)\n",
        "        else:\n",
        "          # directly use result from lstm as query\n",
        "          self.query = decoder_context\n",
        "\n",
        "        if add_activate:\n",
        "          self.key = self.leakyrelu(self.key)\n",
        "          self.value = self.leakyrelu(self.value)\n",
        "          self.query = self.leakyrelu(self.query)\n",
        "\n",
        "        self.key = self.key.to(DEVICE)\n",
        "        raw_weights = torch.bmm(self.key, self.query.unsqueeze(2)).squeeze(2).to(DEVICE)\n",
        "        # key shape: batch, timesteps, project\n",
        "        # query unsqueeze: batch, project, 1 >>> batch, timesteps, 1\n",
        "        if attn_mask is not None:\n",
        "           # a mask with shape (batch, timesteps)\n",
        "          attn_mask = attn_mask.to(DEVICE)\n",
        "          to_fill = -1e9 if raw_weights.dtype == torch.float32 else -1e+4\n",
        "          raw_weights.masked_fill_(attn_mask, to_fill)\n",
        "        else:\n",
        "          # if no mask, use a factor to smooth the weights\n",
        "          raw_weights = 1/np.sqrt(self.query.shape[1]) * raw_weights\n",
        "\n",
        "        attention_weights = torch.nn.functional.softmax(raw_weights, dim=1)\n",
        "        #What makes raw_weights -> attention_weights\n",
        "        attention_context = torch.bmm(attention_weights.unsqueeze(1), self.value).squeeze(1) # batch, 1, projection\n",
        "\n",
        "        return attention_context, attention_weights"
      ],
      "metadata": {
        "id": "771TXxn7ViOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Speller\n",
        "\n",
        "Similar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n",
        "\n",
        "\n",
        "What you have coded till now:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>\n",
        "\n",
        "For the Speller, what we have to code:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "4Sp1WywZmm1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, attender, input_size, embed_size, speller_hidden_size, direct_query=True):\n",
        "        super(Speller, self).__init__()\n",
        "        self.attender = attender\n",
        "        self.projection_size = self.attender.projection_size\n",
        "        self.attention = attender\n",
        "        self.max_timesteps = 550\n",
        "        self.direct_query = direct_query\n",
        "\n",
        "        # Embedding layer to convert token to latent space\n",
        "        self.embedding =  torch.nn.Embedding(input_size, embed_size, padding_idx=PAD_TOKEN)\n",
        "        # indicate if use LSTM output as query or not\n",
        "        connect_size = self.projection_size if direct_query else speller_hidden_size\n",
        "        # Create a sequence of LSTM Cells\n",
        "        self.lstm_cells =  torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(embed_size+self.projection_size, speller_hidden_size),\n",
        "            torch.nn.LSTMCell(speller_hidden_size, speller_hidden_size),\n",
        "            torch.nn.LSTMCell(speller_hidden_size, connect_size)\n",
        "        )\n",
        "        self.dropout = PLSTMLockedDropout(0.25)\n",
        "\n",
        "        # For CDN (Feel free to change)\n",
        "        self.output_to_cdn = torch.nn.Linear(connect_size+self.projection_size, 2*embed_size)\n",
        "        self.cdn_activate = torch.nn.Tanh()\n",
        "        self.cdn_to_char = torch.nn.Linear(2*embed_size, embed_size)\n",
        "        self.char_activate = torch.nn.Tanh()\n",
        "        self.char_prob = torch.nn.Linear(embed_size, input_size)\n",
        "\n",
        "        # weight tying\n",
        "        self.char_prob.weight = self.embedding.weight\n",
        "\n",
        "    def lstm_step(self, input_word, hidden_state):\n",
        "\n",
        "      for i in range(len(self.lstm_cells)):\n",
        "            hidden_state[i] = self.lstm_cells[i](input_word, hidden_state[i])\n",
        "            input_word = hidden_state[i][0]\n",
        "            if i == 0:\n",
        "              input_word = self.dropout(input_word.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "      return input_word, hidden_state\n",
        "\n",
        "\n",
        "    def CDN(self, cdn_input):\n",
        "      # Make the CDN here, you can add the output-to-char\n",
        "      cdn_output = self.output_to_cdn(cdn_input)\n",
        "      cdn_output = self.cdn_activate(cdn_output)\n",
        "      cdn_output = self.cdn_to_char(cdn_output)\n",
        "      cdn_output = self.char_activate(cdn_output)\n",
        "      cdn_prob = self.char_prob(cdn_output)\n",
        "\n",
        "      return cdn_prob\n",
        "\n",
        "\n",
        "    def forward(self, encoder_lens, y=None, teacher_forcing_ratio=0.9):\n",
        "\n",
        "\n",
        "        if y is not None: # during test/validation\n",
        "            batch_size, timesteps = y.shape\n",
        "        else:\n",
        "            batch_size = self.attender.key.shape[0]\n",
        "            timesteps = self.max_timesteps\n",
        "            teacher_forcing_ratio = 0\n",
        "\n",
        "        # initial context tensor for time t = 0\n",
        "        # attn_context = torch.zeros((batch_size, self.projection_size))\n",
        "        attn_context = self.attender.value[:, 0, :].squeeze(1)#.to(DEVICE)\n",
        "        # initial prediction output\n",
        "        output_symbol = torch.zeros(batch_size, 1).to(DEVICE)\n",
        "        output_symbol[:] = SOS_TOKEN\n",
        "\n",
        "        # universal attention mask\n",
        "        mask_len = self.attender.key.shape[1]\n",
        "        mask = torch.arange(mask_len).unsqueeze(0) >= encoder_lens.unsqueeze(1)\n",
        "        # (1, timesteps) vs (batch_size, 1) -> (N, timesteps max)\n",
        "\n",
        "        raw_outputs = []\n",
        "        attention_plot = []\n",
        "\n",
        "        # Initialize your hidden_states list here similar to HW4P1\n",
        "        hidden_states_list = [None]*len(self.lstm_cells) #if hidden_states_list == None else hidden_states_list\n",
        "\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            p = np.random.random() # generate a probability p between 0 and 1\n",
        "            if self.training and p < teacher_forcing_ratio and t > 0: # t = 0, cannot use t-1 / previous value, so directly use SOS embedding\n",
        "                char_embed = self.embedding(y.long())[:, t-1]  # Take from y, else draw from probability distribution\n",
        "            else:\n",
        "                char_embed = self.embedding(output_symbol.argmax(dim=-1))\n",
        "            # print(t, char_embed.shape, attn_context.shape)\n",
        "            # Concatenate the character embedding and context from attention, as shown in the diagram\n",
        "            lstm_input = torch.cat([char_embed, attn_context], dim=1)\n",
        "\n",
        "            lstm_output, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n",
        "            print(lstm_output.shape)\n",
        "            # What should we retrieve from forward_step to prepare for the next timestep?\n",
        "            attn_context, attn_weights = self.attender(lstm_output, mask, add_activate=False)\n",
        "            print(attn_context.shape)\n",
        "            cdn_input = torch.cat([lstm_output, attn_context], dim=1)\n",
        "            output_symbol = self.CDN(cdn_input)  # call CDN with cdn_input\n",
        "\n",
        "            # Generate a prediction for this timestep and collect it in output_symbols\n",
        "            raw_outputs.append(output_symbol.unsqueeze(1))\n",
        "            attention_plot.append(attn_weights)\n",
        "\n",
        "        attention_plot = torch.stack(attention_plot, dim=1)\n",
        "        raw_outputs = torch.cat(raw_outputs, dim=1)\n",
        "\n",
        "        return raw_outputs, attention_plot"
      ],
      "metadata": {
        "id": "nFkc6MbnlUPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellerFancy(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, attender, input_size, embed_size, speller_hidden_size, direct_query=True):\n",
        "        super(SpellerFancy, self).__init__()\n",
        "        self.attender = attender\n",
        "        self.projection_size = self.attender.projection_size\n",
        "        self.attention = attender\n",
        "        self.max_timesteps = 550\n",
        "        self.direct_query = direct_query\n",
        "\n",
        "        # Embedding layer to convert token to latent space\n",
        "        self.embedding =  torch.nn.Embedding(input_size, embed_size, padding_idx=PAD_TOKEN)\n",
        "        # indicate if use LSTM output as query or not\n",
        "        connect_size = self.projection_size if direct_query else speller_hidden_size\n",
        "        # Create a sequence of LSTM Cells\n",
        "        self.lstm_cells =  torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(embed_size+self.projection_size, speller_hidden_size),\n",
        "            torch.nn.LSTMCell(speller_hidden_size, speller_hidden_size),\n",
        "            torch.nn.LSTMCell(speller_hidden_size, connect_size)\n",
        "        )\n",
        "        self.dropout = PLSTMLockedDropout(0.25)\n",
        "\n",
        "        # For CDN (Feel free to change)\n",
        "        self.output_to_cdn = torch.nn.Linear(connect_size+self.projection_size, 2*embed_size)\n",
        "        self.cdn_activate = torch.nn.Tanh()\n",
        "        self.cdn_to_char = torch.nn.Linear(2*embed_size, embed_size)\n",
        "        self.char_activate = torch.nn.Tanh()\n",
        "        self.char_prob = torch.nn.Linear(embed_size, input_size)\n",
        "\n",
        "        # weight tying\n",
        "        self.char_prob.weight = self.embedding.weight\n",
        "\n",
        "    def lstm_step(self, input_word, hidden_state):\n",
        "\n",
        "      for i in range(len(self.lstm_cells)):\n",
        "            hidden_state[i] = self.lstm_cells[i](input_word, hidden_state[i])\n",
        "            input_word = hidden_state[i][0]\n",
        "            if i < (len(self.lstm_cells)-1):\n",
        "              input_word = self.dropout(input_word.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "      return input_word, hidden_state\n",
        "\n",
        "\n",
        "    def CDN(self, cdn_input):\n",
        "      # Make the CDN here, you can add the output-to-char\n",
        "      cdn_output = self.output_to_cdn(cdn_input)\n",
        "      cdn_output = self.cdn_activate(cdn_output)\n",
        "      cdn_output = self.cdn_to_char(cdn_output)\n",
        "      cdn_output = self.char_activate(cdn_output)\n",
        "      cdn_prob = self.char_prob(cdn_output)\n",
        "\n",
        "      return cdn_prob\n",
        "\n",
        "\n",
        "    def forward(self, encoder_lens, y=None, teacher_forcing_ratio=0.9):\n",
        "\n",
        "        if y is not None: # during test/validation\n",
        "            batch_size, timesteps = y.shape\n",
        "        else:\n",
        "            batch_size = self.attender.key.shape[0]\n",
        "            timesteps = self.max_timesteps\n",
        "            teacher_forcing_ratio = 0\n",
        "\n",
        "        # initial context tensor for time t = 0\n",
        "        # attn_context = torch.zeros((batch_size, self.projection_size))\n",
        "        attn_context = self.attender.value[:, 0, :].squeeze(1)#.to(DEVICE)\n",
        "        # initial prediction output\n",
        "        output_symbol = torch.zeros(batch_size, 1).to(DEVICE)\n",
        "        output_symbol[:] = SOS_TOKEN\n",
        "\n",
        "        # universal attention mask\n",
        "        mask_len = self.attender.key.shape[1]\n",
        "        mask = torch.arange(mask_len).unsqueeze(0) >= encoder_lens.unsqueeze(1)\n",
        "        # (1, timesteps) vs (batch_size, 1) -> (N, timesteps max)\n",
        "\n",
        "        raw_outputs = []\n",
        "        attention_plot = []\n",
        "\n",
        "        # Initialize your hidden_states list here similar to HW4P1\n",
        "        hidden_states_list = [None]*len(self.lstm_cells) #if hidden_states_list == None else hidden_states_list\n",
        "\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            p = np.random.random() # generate a probability p between 0 and 1\n",
        "            if self.training and p < teacher_forcing_ratio and t > 0: # t = 0, cannot use t-1 / previous value, so directly use SOS embedding\n",
        "                char_embed = self.embedding(y.long())[:, t-1]  # Take from y, else draw from probability distribution\n",
        "            else:\n",
        "                char_embed = self.embedding(output_symbol.argmax(dim=-1))\n",
        "            # print(t, char_embed.shape, attn_context.shape)\n",
        "\n",
        "            attn_context, _ = self.attender(char_embed.to(DEVICE), mask.to(DEVICE), compute_query=True)\n",
        "            # Concatenate the character embedding and context from attention, as shown in the diagram\n",
        "            lstm_input = torch.cat([char_embed, attn_context], dim=1)\n",
        "\n",
        "            lstm_output, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n",
        "            # What should we retrieve from forward_step to prepare for the next timestep?\n",
        "            attn_context, attn_weights = self.attender(lstm_output, mask, add_activate=False)\n",
        "\n",
        "            cdn_input = torch.cat([lstm_output, attn_context], dim=1)\n",
        "            output_symbol = self.CDN(cdn_input)  # call CDN with cdn_input\n",
        "\n",
        "            # Generate a prediction for this timestep and collect it in output_symbols\n",
        "            raw_outputs.append(output_symbol.unsqueeze(1))\n",
        "            attention_plot.append(attn_weights)\n",
        "\n",
        "        attention_plot = torch.stack(attention_plot, dim=1)\n",
        "        raw_outputs = torch.cat(raw_outputs, dim=1)\n",
        "\n",
        "        return raw_outputs, attention_plot"
      ],
      "metadata": {
        "id": "LRuemXqWhw2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAS"
      ],
      "metadata": {
        "id": "bgOQlDRI-E4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we finally build the LAS model, comibining the listener, attender and speller together, we have given a template, but you are free to read the paper and implement it yourself."
      ],
      "metadata": {
        "id": "ZuAjQFlTBVED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LAS(torch.nn.Module):\n",
        "  def __init__(self, input_size, encoder_embed_size, encoder_hidden_size, decoder_embed_size, decoder_hidden_size, projection_size=128):\n",
        "      super(LAS, self).__init__()\n",
        "\n",
        "      self.listener = Listener(input_size, encoder_embed_size, encoder_hidden_size)\n",
        "      self.attender = Attention(encoder_hidden_size*2, decoder_hidden_size, projection_size)\n",
        "      self.speller = SpellerFancy(self.attender, len(VOCAB), decoder_embed_size, decoder_hidden_size)\n",
        "\n",
        "  def forward(self, x, lx, y=None, teacher_forcing_ratio=0.9):\n",
        "      # Encode speech features\n",
        "      encoder_outputs, encoder_lens = self.listener(x, lx)\n",
        "      # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n",
        "      # Set keys and values using the encoder outputs\n",
        "      key, value = self.attender.set_key_value(encoder_outputs)\n",
        "      # Decode text with the speller using context from the attention\n",
        "      raw_outputs, attention_plots = self.speller(encoder_lens, y=y, teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "      return raw_outputs, attention_plots\n",
        ""
      ],
      "metadata": {
        "id": "scvB2cI-OSof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Setup"
      ],
      "metadata": {
        "id": "bPZD3vqdUisj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512\n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this\n",
        "\n",
        "model = LAS(\n",
        "    input_size=27,\n",
        "    encoder_embed_size= 256, #384,\n",
        "    encoder_hidden_size= 256, #384,\n",
        "    decoder_embed_size= 256, #384, # 512\n",
        "    decoder_hidden_size=512,\n",
        "    projection_size= 128) #256)\n",
        "    # Initialize your model\n",
        "    # Read the paper and think about what dimensions should be used\n",
        "    # You can experiment on these as well, but they are not requried for the early submission\n",
        "    # Remember that if you are using weight tying, some sizes need to be the same\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "#model.apply(init_weights)\n",
        "print(model)\n",
        "\n",
        "summary(model, x.to(DEVICE), x_len, y.to(DEVICE))"
      ],
      "metadata": {
        "id": "a9LN0l5VUk_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cadf288-0019-435b-e715-16f98cd51365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAS(\n",
            "  (listener): Listener(\n",
            "    (embedding): Sequential(\n",
            "      (0): Conv1d(27, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (lstm): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (dropout): PLSTMLockedDropout()\n",
            "      )\n",
            "      (1): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (dropout): PLSTMLockedDropout()\n",
            "      )\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "        (dropout): PLSTMLockedDropout()\n",
            "      )\n",
            "      (3): PLSTMLockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (attender): Attention(\n",
            "    (VW): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (KW): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.25)\n",
            "  )\n",
            "  (speller): SpellerFancy(\n",
            "    (attender): Attention(\n",
            "      (VW): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (KW): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (leakyrelu): LeakyReLU(negative_slope=0.25)\n",
            "    )\n",
            "    (attention): Attention(\n",
            "      (VW): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (KW): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (leakyrelu): LeakyReLU(negative_slope=0.25)\n",
            "    )\n",
            "    (embedding): Embedding(31, 256, padding_idx=0)\n",
            "    (lstm_cells): Sequential(\n",
            "      (0): LSTMCell(384, 512)\n",
            "      (1): LSTMCell(512, 512)\n",
            "      (2): LSTMCell(512, 128)\n",
            "    )\n",
            "    (dropout): PLSTMLockedDropout()\n",
            "    (output_to_cdn): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (cdn_activate): Tanh()\n",
            "    (cdn_to_char): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (char_activate): Tanh()\n",
            "    (char_prob): Linear(in_features=256, out_features=31, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================================================================\n",
            "                                                 Kernel Shape  \\\n",
            "Layer                                                           \n",
            "0_listener.embedding.Conv1d_0                    [27, 256, 3]   \n",
            "1_listener.embedding.BatchNorm1d_1                      [256]   \n",
            "2_listener.LSTM_lstm                                        -   \n",
            "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout             -   \n",
            "4_listener.pBLSTMs.0.LSTM_blstm                             -   \n",
            "...                                                       ...   \n",
            "4013_speller.Linear_output_to_cdn                  [256, 512]   \n",
            "4014_speller.Tanh_cdn_activate                              -   \n",
            "4015_speller.Linear_cdn_to_char                    [512, 256]   \n",
            "4016_speller.Tanh_char_activate                             -   \n",
            "4017_speller.Linear_char_prob                       [256, 31]   \n",
            "\n",
            "                                                      Output Shape     Params  \\\n",
            "Layer                                                                           \n",
            "0_listener.embedding.Conv1d_0                     [192, 256, 2936]    20.736k   \n",
            "1_listener.embedding.BatchNorm1d_1                [192, 256, 2936]      512.0   \n",
            "2_listener.LSTM_lstm                                 [127902, 512]  1.052672M   \n",
            "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout  [192, 1468, 1024]          -   \n",
            "4_listener.pBLSTMs.0.LSTM_blstm                       [63903, 512]  2.625536M   \n",
            "...                                                            ...        ...   \n",
            "4013_speller.Linear_output_to_cdn                       [192, 512]          -   \n",
            "4014_speller.Tanh_cdn_activate                          [192, 512]          -   \n",
            "4015_speller.Linear_cdn_to_char                         [192, 256]          -   \n",
            "4016_speller.Tanh_char_activate                         [192, 256]          -   \n",
            "4017_speller.Linear_char_prob                            [192, 31]          -   \n",
            "\n",
            "                                                  Mult-Adds  \n",
            "Layer                                                        \n",
            "0_listener.embedding.Conv1d_0                    60.880896M  \n",
            "1_listener.embedding.BatchNorm1d_1                    256.0  \n",
            "2_listener.LSTM_lstm                              1.048576M  \n",
            "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout           -  \n",
            "4_listener.pBLSTMs.0.LSTM_blstm                    2.62144M  \n",
            "...                                                     ...  \n",
            "4013_speller.Linear_output_to_cdn                  131.072k  \n",
            "4014_speller.Tanh_cdn_activate                            -  \n",
            "4015_speller.Linear_cdn_to_char                    131.072k  \n",
            "4016_speller.Tanh_char_activate                           -  \n",
            "4017_speller.Linear_char_prob                        7.936k  \n",
            "\n",
            "[4018 rows x 4 columns]\n",
            "-------------------------------------------------------------------------------------------------------\n",
            "                            Totals\n",
            "Total params            13.629727M\n",
            "Trainable params        13.629727M\n",
            "Non-trainable params           0.0\n",
            "Mult-Adds             1.721835776G\n",
            "=======================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Kernel Shape  \\\n",
              "Layer                                                           \n",
              "0_listener.embedding.Conv1d_0                    [27, 256, 3]   \n",
              "1_listener.embedding.BatchNorm1d_1                      [256]   \n",
              "2_listener.LSTM_lstm                                        -   \n",
              "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout             -   \n",
              "4_listener.pBLSTMs.0.LSTM_blstm                             -   \n",
              "...                                                       ...   \n",
              "4013_speller.Linear_output_to_cdn                  [256, 512]   \n",
              "4014_speller.Tanh_cdn_activate                              -   \n",
              "4015_speller.Linear_cdn_to_char                    [512, 256]   \n",
              "4016_speller.Tanh_char_activate                             -   \n",
              "4017_speller.Linear_char_prob                       [256, 31]   \n",
              "\n",
              "                                                      Output Shape     Params  \\\n",
              "Layer                                                                           \n",
              "0_listener.embedding.Conv1d_0                     [192, 256, 2936]    20736.0   \n",
              "1_listener.embedding.BatchNorm1d_1                [192, 256, 2936]      512.0   \n",
              "2_listener.LSTM_lstm                                 [127902, 512]  1052672.0   \n",
              "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout  [192, 1468, 1024]        NaN   \n",
              "4_listener.pBLSTMs.0.LSTM_blstm                       [63903, 512]  2625536.0   \n",
              "...                                                            ...        ...   \n",
              "4013_speller.Linear_output_to_cdn                       [192, 512]        NaN   \n",
              "4014_speller.Tanh_cdn_activate                          [192, 512]        NaN   \n",
              "4015_speller.Linear_cdn_to_char                         [192, 256]        NaN   \n",
              "4016_speller.Tanh_char_activate                         [192, 256]        NaN   \n",
              "4017_speller.Linear_char_prob                            [192, 31]        NaN   \n",
              "\n",
              "                                                  Mult-Adds  \n",
              "Layer                                                        \n",
              "0_listener.embedding.Conv1d_0                    60880896.0  \n",
              "1_listener.embedding.BatchNorm1d_1                    256.0  \n",
              "2_listener.LSTM_lstm                              1048576.0  \n",
              "3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout         NaN  \n",
              "4_listener.pBLSTMs.0.LSTM_blstm                   2621440.0  \n",
              "...                                                     ...  \n",
              "4013_speller.Linear_output_to_cdn                  131072.0  \n",
              "4014_speller.Tanh_cdn_activate                          NaN  \n",
              "4015_speller.Linear_cdn_to_char                    131072.0  \n",
              "4016_speller.Tanh_char_activate                         NaN  \n",
              "4017_speller.Linear_char_prob                        7936.0  \n",
              "\n",
              "[4018 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-438052e7-e52f-443c-b898-08efede16313\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_listener.embedding.Conv1d_0</th>\n",
              "      <td>[27, 256, 3]</td>\n",
              "      <td>[192, 256, 2936]</td>\n",
              "      <td>20736.0</td>\n",
              "      <td>60880896.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_listener.embedding.BatchNorm1d_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[192, 256, 2936]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_listener.LSTM_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[127902, 512]</td>\n",
              "      <td>1052672.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_listener.pBLSTMs.0.PLSTMLockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[192, 1468, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_listener.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[63903, 512]</td>\n",
              "      <td>2625536.0</td>\n",
              "      <td>2621440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4013_speller.Linear_output_to_cdn</th>\n",
              "      <td>[256, 512]</td>\n",
              "      <td>[192, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4014_speller.Tanh_cdn_activate</th>\n",
              "      <td>-</td>\n",
              "      <td>[192, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4015_speller.Linear_cdn_to_char</th>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>[192, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4016_speller.Tanh_char_activate</th>\n",
              "      <td>-</td>\n",
              "      <td>[192, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4017_speller.Linear_char_prob</th>\n",
              "      <td>[256, 31]</td>\n",
              "      <td>[192, 31]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7936.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4018 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-438052e7-e52f-443c-b898-08efede16313')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-438052e7-e52f-443c-b898-08efede16313 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-438052e7-e52f-443c-b898-08efede16313');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function, Optimizers, Scheduler"
      ],
      "metadata": {
        "id": "23DMfXsaU6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr']) #, weight_decay=5e-5) # TODO: Define the optimizer. Adam/AdamW usually works good for this HW\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "#check how would you fill these values : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n",
        "\n",
        "# Optional (but Recommended): Create a custom class for a Teacher Force Schedule"
      ],
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Levenshtein Distance"
      ],
      "metadata": {
        "id": "ZWQnB8lUVY4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[int(i)])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab=VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "\n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above and uncomment below for toy dataset, as the toy dataset has a list of phonemes to compare\n",
        "        #dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example:\n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "\n",
        "    dist/=batch_size\n",
        "    return dist"
      ],
      "metadata": {
        "id": "rSsiCdxPVeZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Validation functions\n"
      ],
      "metadata": {
        "id": "Pu4MrSMUUIyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate, augment=False):\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \"\"\"\n",
        "    augmentations  = torch.nn.Sequential(\n",
        "          tat.FrequencyMasking(freq_mask_param=10),\n",
        "          tat.TimeMasking(time_mask_param=10)\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "        #if augment:\n",
        "          #x = augmentations(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          # Predictions are of Shape (batch_size, timesteps, vocab_size).\n",
        "          # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "          # So in total, you have batch_size*timesteps amount of characters.\n",
        "          # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "          raw_predictions, attention_plot = model(x, lx, y = y, teacher_forcing_ratio=teacher_forcing_rate)\n",
        "\n",
        "          # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "          # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with\n",
        "          # your implementation\n",
        "          loss = criterion(raw_predictions.view(-1, raw_predictions.shape[2]), y.view(-1))\n",
        "\n",
        "          # create mask for controling the length so as to use reduction = none in criterion\n",
        "          len_mask = torch.zeros(y.shape[1], y.shape[0])\n",
        "          len_mask = len_mask.to(DEVICE)\n",
        "          for j in range(len(ly)):\n",
        "            len_mask[:ly[j], j] = 1\n",
        "\n",
        "          m = len_mask.flatten()\n",
        "\n",
        "          adj_loss = torch.sum(loss*m) / torch.sum(len_mask)\n",
        "          perplexity = torch.exp(adj_loss).item() # exponential of the loss\n",
        "\n",
        "          running_loss += adj_loss.item()\n",
        "          running_perplexity += perplexity\n",
        "\n",
        "        if DEVICE == 'cuda':\n",
        "          scaler.scale(adj_loss).backward()\n",
        "\n",
        "          # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n",
        "          scaler.unscale_(optimizer)\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "\n",
        "          # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "          # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "          scaler.step(optimizer)\n",
        "          scaler.update()\n",
        "        else:\n",
        "          adj_loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ],
      "metadata": {
        "id": "sKOdI0J5Tpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  torch.argmax(raw_predictions, dim=2) #greedy_predict(raw_predictions)\n",
        "        # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist"
      ],
      "metadata": {
        "id": "YmBLhP8cWm6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "JmZhxhNseaIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Wandb\n",
        "# Initialize your Wandb Run Here\n",
        "# Save your model architecture in a txt file, and save the file to Wandb\n",
        "import wandb\n",
        "wandb.login(key=\"27ad915a9386068b1fc160cd97b84be7ba1fe659\")"
      ],
      "metadata": {
        "id": "sZcCV2BIW2R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9851c0-4e90-4be0-d190-0ce07ecbd58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxinz3\u001b[0m (\u001b[33msharonxin1207\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOG_RUN = True\n",
        "if LOG_RUN:\n",
        "  run = wandb.init(\n",
        "      name = \"run-hw4-complete\", ## Wandb creates random run names if you skip this field\n",
        "      reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "      # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "      #resume = \"allow\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "      project = \"hw4p2-ablations\", ### Project should be created in your wandb account\n",
        "      config = config ### Wandb Config for your run\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "zch_XsXQ0nY-",
        "outputId": "aa545a2a-e298-4ec7-c41e-15ab319195e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.15.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_135337-lv9r614w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sharonxin1207/hw4p2-ablations/runs/lv9r614w' target=\"_blank\">run-1+3+3+2-finetune2</a></strong> to <a href='https://wandb.ai/sharonxin1207/hw4p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/sharonxin1207/hw4p2-ablations' target=\"_blank\">https://wandb.ai/sharonxin1207/hw4p2-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/sharonxin1207/hw4p2-ablations/runs/lv9r614w' target=\"_blank\">https://wandb.ai/sharonxin1207/hw4p2-ablations/runs/lv9r614w</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention):\n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ],
      "metadata": {
        "id": "IgJdA9jrZwid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model path for saving\n",
        "epoch_model_path = \"hw4_epoch.pt\" #TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = \"hw4_best.pt\" #TODO set best model path\n",
        "# \"best_fancy_speller.pt\" >>> best 11.6\n",
        "#\"best_lstm_query_model_adjtf.pt\" #TODO set best model path\n",
        "reload_path =   \"hw4_models/1+3+3+2_best.pt\"#\"best_fancy_speller.pt\""
      ],
      "metadata": {
        "id": "X8n3_-fj2O2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RELOAD = False\n",
        "if RELOAD:\n",
        "  reload_items =  load_model(best_model_path, model, 'valid_dist', optimizer, scheduler)\n",
        "  model = reload_items[0]\n",
        "  optimizer = reload_items[1]\n",
        "  scheduler = reload_items[2]\n",
        "  print(reload_items[3], reload_items[4])\n",
        "  print(optimizer.param_groups[0]['lr'])"
      ],
      "metadata": {
        "id": "9w0aRzI0nArz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8014ad06-dad0-4590-a4e9-803a4f9f2bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "113 7.5995138888888905\n",
            "0.0001099511627776001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "best_lev_dist = float(\"inf\") # reload_items[4]\n",
        "tf_rate = 1\n",
        ""
      ],
      "metadata": {
        "id": "tp2s7N-4FBj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training for 100 Epochs"
      ],
      "metadata": {
        "id": "R0lLBgAwzXYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss, train_perplexity, attention_plot    = train(model, train_loader, criterion, optimizer, tf_rate) #TODO\n",
        "    valid_dist              = validate(model, dev_loader) #TODO\n",
        "    if epoch > 14:\n",
        "      scheduler.step(valid_dist)\n",
        "    # Print your metrics\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Train Perplexity  {:.04f}\\t Learning Rate {:.07f}\".format(\n",
        "        train_loss, train_perplexity, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t\".format(valid_dist))\n",
        "\n",
        "    # Plot Attention for a single item in the batch\n",
        "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
        "    # one schedule is reduce at 20, then every 15 epoch until 0.1\n",
        "    if ((epoch+1)%20 == 0) and tf_rate > 0.4 and epoch < 60:\n",
        "      tf_rate -= 0.1\n",
        "      print(\"Reducing TF rate by 0.1\")\n",
        "    # Log metrics to Wandb\n",
        "    elif ((epoch+1) == 75) and tf_rate > 0.4:\n",
        "      tf_rate -= 0.1\n",
        "\n",
        "      print(\"Reducing TF rate by 0.1\")\n",
        "    \"\"\"\n",
        "    elif epoch+1 == 50 and tf_rate > 0.1:\n",
        "      tf_rate -= 0.05\n",
        "      print(\"Reducing TF rate by 0.05\")\n",
        "    elif epoch+1 == 60 and tf_rate > 0.1:\n",
        "      tf_rate -= 0.05\n",
        "      print(\"Reducing TF rate by 0.05\")\n",
        "    elif epoch+1 == 70 and tf_rate > 0.1:\n",
        "      tf_rate -= 0.05\n",
        "      print(\"Reducing TF rate by 0.05\")\"\"\"\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'train_perplex': train_perplexity,\n",
        "        'valid_dist': valid_dist,\n",
        "        'tf_rate': tf_rate,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    wandb.save(epoch_model_path)\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        #wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "run.finish()\n"
      ],
      "metadata": {
        "id": "eDWGFIcjddz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning for 50 epochs"
      ],
      "metadata": {
        "id": "ica8lg1ezgzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_RUN:\n",
        "  run = wandb.init(\n",
        "      name = \"run-hw4-complete-finetune\", ## Wandb creates random run names if you skip this field\n",
        "      reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "      # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "      #resume = \"allow\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "      project = \"hw4p2-ablations\", ### Project should be created in your wandb account\n",
        "      config = config ### Wandb Config for your run\n",
        "  )"
      ],
      "metadata": {
        "id": "uJgJ7unaW-GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_model_path = \"hw4_ft_epoch.pt\" #TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = \"hw4_ft_best.pt\" #TODO set best model path"
      ],
      "metadata": {
        "id": "nMrWfsZtXIFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset LR and scheduler for finetune\n",
        "optimizer.param_groups[0]['lr'] = 0.0005\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=3, verbose=True)\n"
      ],
      "metadata": {
        "id": "l0ABxbpHo1Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(config['finetune_epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['finetune_epochs']))\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss, train_perplexity, attention_plot    = train(model, train_loader, criterion, optimizer, tf_rate) #TODO\n",
        "    valid_dist              = validate(model, dev_loader) #TODO\n",
        "    if epoch > 14:\n",
        "      scheduler.step(valid_dist)\n",
        "    # Print your metrics\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Train Perplexity  {:.04f}\\t Learning Rate {:.07f}\".format(\n",
        "        train_loss, train_perplexity, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t\".format(valid_dist))\n",
        "\n",
        "    # Plot Attention for a single item in the batch\n",
        "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
        "    # one schedule is reduce at 20, then every 15 epoch until 0.1\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'train_perplex': train_perplexity,\n",
        "        'valid_dist': valid_dist,\n",
        "        'tf_rate': tf_rate,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    wandb.save(epoch_model_path)\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        #wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "run.finish()\n"
      ],
      "metadata": {
        "id": "_JVCt7wZzclw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "hgFYFaBGeBqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Load your best model Checkpoint here\n",
        "\n",
        "# TODO: Create a testing function similar to validation\n",
        "# TODO: Create a file with all predictions\n",
        "# TODO: Submit to Kaggle\n",
        "\n",
        "def prediction_decode(to_decode):\n",
        "    # to_decode: batch_size, seq_len, vocab_size\n",
        "    decode = []\n",
        "    for seq in to_decode:\n",
        "        seq_str = \"\"\n",
        "        for char in seq:\n",
        "            if char == EOS_TOKEN:\n",
        "              break\n",
        "            elif char == SOS_TOKEN:\n",
        "              pass\n",
        "            else:\n",
        "              seq_str += VOCAB[char]\n",
        "        decode.append(seq_str)\n",
        "    return decode\n",
        "\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    #batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    pred_list = []\n",
        "\n",
        "    for (x, lx) in tqdm(dataloader):\n",
        "        x, lx = x.to(DEVICE), lx\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "        greedy_predictions = prediction_decode(torch.argmax(raw_predictions, dim=2))\n",
        "        pred_list.extend(greedy_predictions)\n",
        "\n",
        "        del x, lx\n",
        "        torch.cuda.empty_cache()\n",
        "    return pred_list\n",
        ""
      ],
      "metadata": {
        "id": "hw2PPKXCdN4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction = test(model, test_loader)\n",
        "with open(\"submission.csv\", 'w') as fh:\n",
        "  fh.write('index,label\\n')\n",
        "  for i in range(len(test_prediction)):\n",
        "    fh.write(str(i)+ ',' + test_prediction[i] + \"\\n\")\n",
        "\n",
        "df = pd.read_csv(\"submission.csv\")\n",
        "df.to_csv('submission.csv', index = False)\n"
      ],
      "metadata": {
        "id": "qo25BQGhhDxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83480cc-4272-4e54-e9a1-6d9b90a40b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/14 [00:03<00:51,  3.99s/it]\u001b[A\n",
            " 14%|█▍        | 2/14 [00:07<00:41,  3.42s/it]\u001b[A\n",
            " 21%|██▏       | 3/14 [00:09<00:34,  3.17s/it]\u001b[A\n",
            " 29%|██▊       | 4/14 [00:12<00:30,  3.03s/it]\u001b[A\n",
            " 36%|███▌      | 5/14 [00:15<00:27,  3.01s/it]\u001b[A\n",
            " 43%|████▎     | 6/14 [00:19<00:25,  3.14s/it]\u001b[A\n",
            " 50%|█████     | 7/14 [00:21<00:21,  3.03s/it]\u001b[A\n",
            " 57%|█████▋    | 8/14 [00:24<00:17,  2.97s/it]\u001b[A\n",
            " 64%|██████▍   | 9/14 [00:27<00:14,  3.00s/it]\u001b[A\n",
            " 71%|███████▏  | 10/14 [00:30<00:11,  2.88s/it]\u001b[A\n",
            " 79%|███████▊  | 11/14 [00:33<00:08,  2.89s/it]\u001b[A\n",
            " 86%|████████▌ | 12/14 [00:36<00:05,  2.99s/it]\u001b[A\n",
            " 93%|█████████▎| 13/14 [00:39<00:02,  2.98s/it]\u001b[A\n",
            "100%|██████████| 14/14 [00:41<00:00,  2.99s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if SUBMIT:\n",
        "  !kaggle competitions submit -c 11-785-s23-hw4p2 -f submission.csv -m \"I made it!\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3l8w1xBdl5t",
        "outputId": "a9e9cd70-dc7c-4501-b9ae-e197fa631c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 288k/288k [00:00<00:00, 601kB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ]
    }
  ]
}